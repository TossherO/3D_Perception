{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-cuxlzjsv because the default path (/home/hello/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "os.chdir('../')\n",
    "from mmcv.cnn.bricks.transformer import MultiheadAttention\n",
    "from flash_attn.flash_attn_interface import flash_attn_varlen_kvpacked_func\n",
    "from my_projects.CMT.cmt.models.utils.flash_attention import FlashMHA\n",
    "from mmcv.ops.multi_scale_deform_attn import MultiScaleDeformableAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the regular attention layer\n",
    "regular_attention = MultiheadAttention(embed_dims=256, num_heads=8).cuda()\n",
    "\n",
    "# Create an instance of the deformable attention layer\n",
    "deformable_attention = MultiScaleDeformableAttention(embed_dims=256, num_heads=8).cuda()\n",
    "\n",
    "# Create an instance of the flash attention layer\n",
    "flash_attention = FlashMHA(embed_dim=256, num_heads=8).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3918044567108154\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    query = torch.randn(1024, 1, 256, device=torch.device('cuda'))\n",
    "    attn_mask = torch.zeros(1024, 1024, device=torch.device('cuda')).bool()\n",
    "    for _ in range(1000):\n",
    "        out = regular_attention(query, attn_mask=attn_mask)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9290800094604492\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    query = torch.randn(1024, 1, 256, device=torch.device('cuda'))\n",
    "    reference_points = torch.randn(16, 1024, 1, 2, device=torch.device('cuda'))\n",
    "    spatial_shapes = torch.tensor([[32, 32]], device=torch.device('cuda'))\n",
    "    level_start_index = torch.tensor([0], device=torch.device('cuda'))\n",
    "    for _ in range(1000):\n",
    "        out = deformable_attention(query, reference_points=reference_points, \n",
    "            spatial_shapes=spatial_shapes, level_start_index=level_start_index)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9855546951293945\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    query = torch.randn(1024, 1, 256, device=torch.device('cuda'))\n",
    "    for _ in range(1000):\n",
    "        out = flash_attention(query, query, query)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
